<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mohammad Ali Zamani</title>
    <link>https://zamani.github.io/</link>
      <atom:link href="https://zamani.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Mohammad Ali Zamani</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zamani.github.io/media/icon_hu1cbb8980017bdde61911f0d050bd5d33_384171_512x512_fill_lanczos_center_3.png</url>
      <title>Mohammad Ali Zamani</title>
      <link>https://zamani.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://zamani.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VERIKAS</title>
      <link>https://zamani.github.io/project/verikas/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/verikas/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Workshop on Reinfocement Learning</title>
      <link>https://zamani.github.io/talk/rl_ws_2021/</link>
      <pubDate>Tue, 25 May 2021 12:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/rl_ws_2021/</guid>
      <description>&lt;p&gt;In collaboration with ARIC, I had my second workshop on reinforcement learning. This workshop was followed by a hands-on session about RL.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IMPA</title>
      <link>https://zamani.github.io/project/impa/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/impa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Continuous Acoustic Emotion Classification using Recurrent Neural Network</title>
      <link>https://zamani.github.io/talk/brown_bag_2020/</link>
      <pubDate>Thu, 13 Aug 2020 12:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/brown_bag_2020/</guid>
      <description>&lt;p&gt;I was invited as speaker to the Remote Brown Bag Sessions which are orginized by Artificial Intelligence Center Hamburg (ARIC). In this talk, I presented our work on Continuous Acoustic Emotion Classification which we could reduce latency and improve the accuracy for critical cases. You can find  the list of talks orginized by ARIC &lt;a href=&#34;https://github.com/aric-hamburg/remote-brown-bag-sessions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNEVIS - AI</title>
      <link>https://zamani.github.io/project/unevis/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/unevis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Workshop on Deep Neural Learning and Bayesian Optimization of Hyperparameters</title>
      <link>https://zamani.github.io/talk/aric2020/</link>
      <pubDate>Thu, 23 Apr 2020 20:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/aric2020/</guid>
      <description>&lt;p&gt;The (online) workshop is  organized   by  Hamburger Informatik Technologie-Center e.V. (HITeC) and Artificial Intelligence Center Hamburg e.V. (ARIC) on 9th, 16th, and 23rd April 2020. The focus of the workshop was on optimizing hyperparameters of deep learning models with Bayesian optimization. In this workshop, participants learned how to write searchable deep learning models in Pytorch and searching for hyperparameters with one of the latest AutoML methods. Bayesian Optimization and HyperBand (&lt;a href=&#34;https://www.automl.org/automl/bohb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;BOHB&lt;/em&gt;&lt;/a&gt;) is introduced for hyperparameter optimization. On each day, there was a practical session for implementing the concepts on Google Colaboratory.&lt;/p&gt;
&lt;p&gt;The content of the workshop was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Day 1 (09.04.2020) Introduction to Neural Network
&lt;ul&gt;
&lt;li&gt;Basics of Neural Network&lt;/li&gt;
&lt;li&gt;Basics of Pytorch&lt;/li&gt;
&lt;li&gt;Multi-Layer Perceptron&lt;/li&gt;
&lt;li&gt;Hands-on Session&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Day 2 (16.04.2020) Convolutional Neural Network
&lt;ul&gt;
&lt;li&gt;Questions from the previous session&lt;/li&gt;
&lt;li&gt;Basics of Convolutional Neural Network&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network in Pytorch&lt;/li&gt;
&lt;li&gt;Hands-on Session&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Day 3 (23.04.2020) Bayesian Optimization
&lt;ul&gt;
&lt;li&gt;Questions from the previous session&lt;/li&gt;
&lt;li&gt;Introduction of Bayesian Optimization&lt;/li&gt;
&lt;li&gt;Introduction of BOHB Framework&lt;/li&gt;
&lt;li&gt;Writing a Searchable Architecture&lt;/li&gt;
&lt;li&gt;Hands-on Session&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on Deep Learning with Pytorch and AutoML</title>
      <link>https://zamani.github.io/talk/hitec_automl_2020/</link>
      <pubDate>Fri, 07 Feb 2020 10:30:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/hitec_automl_2020/</guid>
      <description>&lt;p&gt;My second internal workshop at HITeC was about deep learning with Pytorch and AutoML. I introduced the basics of deep learning and how to implement a classification problem in Pytorch. Then, I introduced one of the latest AutoML approach called &lt;a href=&#34;https://www.automl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;BOHB&lt;/em&gt;&lt;/a&gt; for hyperparameter optimization. At the end of the workshop, we had hands-on session for hyperparameter search for a 2-D classification problem. The content of the workshop was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning with Pytorch&lt;/li&gt;
&lt;li&gt;AutoML&lt;/li&gt;
&lt;li&gt;Hands-on Session&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bike sharing prediction using deep learning</title>
      <link>https://zamani.github.io/post/bike_sharing_project/</link>
      <pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/post/bike_sharing_project/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h1 id=&#34;bike-sharing&#34;&gt;bike-sharing&lt;/h1&gt;
&lt;p&gt;The goal is to predict the bike sharing count per hour. The dataset can be downloaded &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Bike&amp;#43;Sharing&amp;#43;Dataset.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have uploaded the source code in this repo:
&lt;a href=&#34;https://github.com/zamani/bike_sharing_prediction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/zamani/bike_sharing_prediction&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;⚠️update pip and install virtualenv if necessary (check &lt;a href=&#34;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;)
(Optional) update the python pip&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First clone the repo and navigate into the local repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/zamani/bike_sharing_prediction.git
cd bike_sharing_prediction
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a virtual environment in python:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m venv bike_sharing_env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, activate the environment on Windows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.\bike_sharing_env\Scripts\activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(And on Linux/macOS)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, install the requirements:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;baselines&#34;&gt;Baselines&lt;/h2&gt;
&lt;p&gt;The proposed RNN method is compared with some simple baseline methods:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Baseline Method&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;MAE&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;STD&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Previous Hour Count&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;85.06&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;97.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Previous Day, Same Hour&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;81.08&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;108.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Keep Hourly Pace&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;89.21&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;125.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Linear Regression, without regularization&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;142.21&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;115.59&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results can be shown by running the following command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python baselines.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;In this repository, I have used RNN-GRU with different configurations. The RNN architecture is chosen for this problem because considering the sequence of consequence features and the previous output gives us additional information (comparing with only current input features). This leads to a more accurate prediction and can be modeled by the RNN. First, the dataset is divided into 70% training set, 10% validation set and 20% test set. Then, the bike count is normalized respect to the maximum bike count number in the training set.&lt;/p&gt;
&lt;p&gt;The network is trained with the relative count respect to the previous day (same hour) or the previous hour. It means in this approach we have one of these two assumptions: either (1) we already know the count of the previous hour and we need to predict the next hour or (2) we know the previous day data (which is more realistic). The proposed architecture calculates the relative change and its value is converted into the absolute value for getting the absolute count number. Also, the relative change was trained by converting them into discrete bins and use the representative of bins as the relative increase/decrease. Only the number of bins should be defined. The bins width are automatically selected in a way that each bin almost has the same number of samples. On the other hands bins can also be defined manually such as [-1]+[-.75:0.05:0.45]+[0.5:0.25:2.5].&lt;/p&gt;
&lt;p&gt;Here are the results with Gated Recurrent Unit (GRU) on a fixed random seed:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;RNN-GRU input (output is Y(t))&lt;/th&gt;
&lt;th&gt;command (&lt;code&gt;python main.py &amp;lt;ARGUMENTS&amp;gt;&lt;/code&gt;)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;STD&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;X(t)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;82.42&lt;/td&gt;
&lt;td&gt;132.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[X(t-1), X(t)]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;75.53&lt;/td&gt;
&lt;td&gt;140.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[X(t-2), X(t-1), X(t)]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;92.69&lt;/td&gt;
&lt;td&gt;162.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[X(t), Y(t-1)]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 1 -prev_cnt hour&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;43.78&lt;/td&gt;
&lt;td&gt;71.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[[X(t-1), Y(t-2)], [X(t), Y(t-1)]]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 2 -prev_cnt hour&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;13.73&lt;/td&gt;
&lt;td&gt;19.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[X(t), Y(t-24)]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 1 -prev_cnt day -day_num 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;65.95&lt;/td&gt;
&lt;td&gt;108.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[X(t), Y(t-24), Y(t-48)]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 1 -prev_cnt day -day_num 2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;33.15&lt;/td&gt;
&lt;td&gt;33.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[[X(t-1), Y(t-25)], [X(t), Y(t-24)]]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 2 -prev_cnt day -day_num 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64.24&lt;/td&gt;
&lt;td&gt;100.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[[X(t-1), Y(t-25), Y(t-49)], [X(t), Y(t-24), Y(t-48)]]&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-seqlen 2 -prev_cnt day -day_num 2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;18.98&lt;/td&gt;
&lt;td&gt;34.17&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The predicted value and actual value for a portion of test set is shown in the figure below (with this configuration
[[X(t-1), Y(t-25), Y(t-49)], [X(t), Y(t-24), Y(t-48)]]).&lt;/p&gt;
&lt;!--- ![result](Figure_1.png) --&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Test result&#34;
           src=&#34;https://zamani.github.io/post/bike_sharing_project/test_results.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are other hyperparameters which can be explored such as learning rate, batch size, num of bins, hidden size, number of layers, dropout, and reduced features. Also, different values can be tested for seqlen, day_num.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on Gaussian Process and Bayesian Optimization</title>
      <link>https://zamani.github.io/talk/hitec_gpbo2020/</link>
      <pubDate>Mon, 13 Jan 2020 10:30:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/hitec_gpbo2020/</guid>
      <description>&lt;p&gt;I had an internal workshop at HITeC on Introduction to Gaussian Process and Bayesian Optimization. The content of the workshop was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basics from Statistics&lt;/li&gt;
&lt;li&gt;Gaussian Process&lt;/li&gt;
&lt;li&gt;Bayesian Optimization&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SECURE Project</title>
      <link>https://zamani.github.io/project/secure/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/secure/</guid>
      <description>&lt;p&gt;I was part of the SECURE project between 2016-2019. Here is The main page for the &lt;a href=&#34;https://secure-robots.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;SECURE project&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://zamani.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep reinforcement learning using compositional representations for performing instructions</title>
      <link>https://zamani.github.io/publication/paladyn/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/paladyn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Incorporating End-to-End Speech Recognition Models for Sentiment Analysis</title>
      <link>https://zamani.github.io/publication/icra2019/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/icra2019/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/g1Xkq2AUGl8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>1) European Researchers&#39; Night:</title>
      <link>https://zamani.github.io/outreach/msca/</link>
      <pubDate>Wed, 05 Dec 2018 01:05:05 +0100</pubDate>
      <guid>https://zamani.github.io/outreach/msca/</guid>
      <description>&lt;p&gt;We demonstrated our project &amp;quot; robot that learns how to perform language instructions&amp;quot; in  ‘Science is Wonder-ful!’, on September 26-27 2018. Our project shows the ability of robots to understand spoken instructions which was working a noisy condition.You can see more details &lt;a href=&#34;https://zamani.github.io/project/instructing_arm_robot/&#34;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca3.png&#34; alt=&#34;&amp;amp;quot;(https://ec.europa.eu/research/mariecurieactions/)&amp;amp;quot;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

I explained our project to Dimitrios Papadimoulis, Vice President of the European Parliament. (Image credit: &lt;a href=&#34;https://ec.europa.eu/research/mariecurieactions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;European Comission - MSCA&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca0.png&#34; alt=&#34;Mohammad Ali Zamani, Robot that understands spoken instructions&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca1.jpg&#34; alt=&#34;Mohammad Ali Zamani, Robot that understands spoken instructions&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca2.jpg&#34; alt=&#34;Mohammad Ali Zamani, Robot that understands spoken instructions&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca4.jpg&#34; alt=&#34;Mohammad Ali Zamani, Robot that understands spoken instructions&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/msca5.png&#34; alt=&#34;Mohammad Ali Zamani, Robot that understands spoken instructions&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You can find the video here:
&lt;video src=&#34;https://ec.europa.eu/research/mariecurieactions/sites/mariecurie2/files/science-is-wonderful-video-2018_3.mp4&#34; width=&#34;840&#34; controls&gt;
&lt;/video&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2 poster presentations at IROS 2018</title>
      <link>https://zamani.github.io/talk/iros2018/</link>
      <pubDate>Fri, 05 Oct 2018 16:30:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/iros2018/</guid>
      <description>&lt;p&gt;I jointly presented our paper &lt;a href=&#34;https://zamani.github.io/publication/iros2018/&#34;&gt;&lt;em&gt;On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks&lt;/em&gt;&lt;/a&gt; during the interactive section. Also, on the last day of the conference, I attended the &lt;em&gt;Machine Learning in Robot Motion Planning&lt;/em&gt; workshop to present our work on &lt;a href=&#34;https://zamani.github.io/publication/iros2018_ws/&#34;&gt;&lt;em&gt;Neural End-to-End Learning of Reach for Grasp Ability with a 6-DoF Robot Arm&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural End-to-End Learning of Reach for Grasp Ability with a 6-DoF Robot Arm</title>
      <link>https://zamani.github.io/publication/iros2018_ws/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/iros2018_ws/</guid>
      <description>













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/iros2018_ws/robot_hu17a3b2f95efd667e8876c6d22114a649_49543_1be3a760d10d96e69c3fc587abf202db.webp 400w,
               /publication/iros2018_ws/robot_hu17a3b2f95efd667e8876c6d22114a649_49543_106bab6b3abb61f97ea311057f077d57.webp 760w,
               /publication/iros2018_ws/robot_hu17a3b2f95efd667e8876c6d22114a649_49543_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zamani.github.io/publication/iros2018_ws/robot_hu17a3b2f95efd667e8876c6d22114a649_49543_1be3a760d10d96e69c3fc587abf202db.webp&#34;
               width=&#34;760&#34;
               height=&#34;233&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks</title>
      <link>https://zamani.github.io/publication/iros2018/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/iros2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language-modulated Actions using Deep Reinforcement Learning for Safer Human-Robot Interaction</title>
      <link>https://zamani.github.io/publication/ssr2018/</link>
      <pubDate>Sat, 29 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/ssr2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Presenting my research progress</title>
      <link>https://zamani.github.io/talk/ssr2018/</link>
      <pubDate>Fri, 28 Sep 2018 14:15:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/ssr2018/</guid>
      <description>&lt;p&gt;We organized a PhD students conference focused safety and interaction quality. I also had a chance to present my &lt;a href=&#34;https://zamani.github.io/publication/ssr2018/&#34;&gt;&lt;em&gt;progress&lt;/em&gt;&lt;/a&gt; in this conference.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Spatial Representation for Safe Human-Robot Collaboration in Joint Manual Tasks</title>
      <link>https://zamani.github.io/publication/icra2018_workmate_ws/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/icra2018_workmate_ws/</guid>
      <description>&lt;p&gt;













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;icra_2018_workmate.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/M9u8IcmKRbI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2) Interview with the MCAA IRRADIUM magazine</title>
      <link>https://zamani.github.io/outreach/irradium/</link>
      <pubDate>Tue, 22 May 2018 15:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/outreach/irradium/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://zamani.github.io/img/interview_irradium.png&#34; alt=&#34;Mohammad Ali Zamani, interview, artificial intelligence&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I was interviewed by the &lt;a href=&#34;https://www.mariecuriealumni.eu/mcaa-magazine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;MCAA IRRADIUM&lt;/em&gt;&lt;/a&gt; magazine about the challenges, development, and uptake of . The online version of the interview can be found here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mariecuriealumni.eu/mcaa-magazine/february-2019/innovative-technologies/european-approach-artificial-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;European approach to Artificial Intelligence: Challenges, developments and uptake&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The PDF version of the magazine can be downloaded &lt;a href=&#34;https://www.mariecuriealumni.eu/sites/default/files/mcaa_magazine_january2019_web_final.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;
(page 16-17)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3 poster presentation at ICRA 2018</title>
      <link>https://zamani.github.io/talk/icra2018/</link>
      <pubDate>Tue, 22 May 2018 15:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/icra2018/</guid>
      <description>&lt;p&gt;I presented 3 posters at ICRA main conference, Workmate workshop and PhD forum.&lt;/p&gt;
&lt;p&gt;The first presentation was &lt;a href=&#34;https://zamani.github.io/publication/emorl/&#34;&gt;&lt;em&gt;“Emorl: Continuous acoustic emotion classification using deep reinforcement learning”&lt;/em&gt;&lt;/a&gt; at ICRA 2018.&lt;/p&gt;
&lt;p&gt;I also presented &lt;a href=&#34;https://zamani.github.io/publication/icra2018_workmate_ws/&#34;&gt;&lt;em&gt;&amp;ldquo;Learning spatial representation for safe human-robot collaboration in joint manual tasks&amp;rdquo;&lt;/em&gt;&lt;/a&gt; at the Workmate workshop at ICRA 2018.&lt;/p&gt;
&lt;p&gt;And finally, I presented my work on &lt;a href=&#34;https://zamani.github.io/publication/icra2018_phd_forum/&#34;&gt;&lt;em&gt;&amp;ldquo;Language-modulated Actions for Safer Human-Robot Interaction using Deep Reinforcement Learning&amp;rdquo;&lt;/em&gt;&lt;/a&gt; as a poster in the ICRA 2018 Ph.D. forum.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EmoRL: Real-time Acoustic Emotion Classification using Deep Reinforcement Learning</title>
      <link>https://zamani.github.io/publication/emorl/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/emorl/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/bheIo0RUwT0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Language-modulated Safer Actions using Deep Reinforcement Learning</title>
      <link>https://zamani.github.io/publication/icra2018_phd_forum/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/icra2018_phd_forum/</guid>
      <description>













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/icra2018_phd_forum/featured1_hu4d4028bc072c7a4602ed1579f61b1b73_71434_202ae26361b65479759198999c26aec8.webp 400w,
               /publication/icra2018_phd_forum/featured1_hu4d4028bc072c7a4602ed1579f61b1b73_71434_fdbfcd886f42e99095236261e5fa9597.webp 760w,
               /publication/icra2018_phd_forum/featured1_hu4d4028bc072c7a4602ed1579f61b1b73_71434_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zamani.github.io/publication/icra2018_phd_forum/featured1_hu4d4028bc072c7a4602ed1579f61b1b73_71434_202ae26361b65479759198999c26aec8.webp&#34;
               width=&#34;421&#34;
               height=&#34;169&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Accelerating Deep Continuous Reinforcement Learning through Task Simplification</title>
      <link>https://zamani.github.io/publication/ijcnn2018/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/ijcnn2018/</guid>
      <description>













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/ijcnn2018/featured_huff265ce9940313990625933b11874ca9_40647_16a309f7d01e03397c7ee8b444f2975c.webp 400w,
               /publication/ijcnn2018/featured_huff265ce9940313990625933b11874ca9_40647_72341124232a1adc3920f6ab702b1773.webp 760w,
               /publication/ijcnn2018/featured_huff265ce9940313990625933b11874ca9_40647_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zamani.github.io/publication/ijcnn2018/featured_huff265ce9940313990625933b11874ca9_40647_16a309f7d01e03397c7ee8b444f2975c.webp&#34;
               width=&#34;516&#34;
               height=&#34;548&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Robot that performs language instructions</title>
      <link>https://zamani.github.io/project/instructing_arm_robot/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/instructing_arm_robot/</guid>
      <description>&lt;p&gt;&lt;em&gt;Click&lt;/em&gt; &lt;a href=&#34;https://zamani.github.io/outreach/&#34;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for more photos&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Spoken language is potentially the most intuitive way to communicate with robot. In this project, we assembled an arm robot to demonstrate to public, especially younger kids, how they can instruct robot to perform actions. This project includes automatic speech recognition (ASR), text to speech, spoken language understanding, vision, and motion planning.&lt;/p&gt;
&lt;p&gt;To work with our robot, a user can wake up the robot by its name &amp;ldquo;Jarvis&amp;rdquo;. Then, the robot confirms that is ready to receive a new command by saying &amp;ldquo;Yes!&amp;rdquo;. After hearing the confirmation, the user give a command such as &amp;ldquo;Pick cube number 5 and put it near cube numebr 1&amp;rdquo;. Although, we had our own domain-specific ASR model, due to operating condition which was in an extremely noisy condition we used the Google ASR. We ask the Google ASR the top most probable hypotheses. These hypotheses are used to identify the intended cubes as well as a target position for the picked cube. If there was no target position is commanded then robot performed a handover when users says &amp;ldquo;Release!&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Using a top camera robot locates the cubes using the April tags on top each cubes and using moveIt! package it plans and performs the trajectory to reach to the cube. Jarvis can grasp the cube using a electomagnet which is installed at the end-effector of the robot. There is also a metal piece is attached to the top of the cubes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Should We Be Afraid of Artificial Intelligence?</title>
      <link>https://zamani.github.io/talk/ham2018/</link>
      <pubDate>Fri, 15 Dec 2017 17:30:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/ham2018/</guid>
      <description>&lt;p&gt;In this presentation, I talked about AI and public concerns.  The purpose of the talk was to give some intuition for a public audience that what is the research focus and methodological tools in AI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning using Symbolic Representation for Performing Spoken Language Instructions</title>
      <link>https://zamani.github.io/publication/roman2017/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/roman2017/</guid>
      <description>













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;featured1.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Simultaneous Human-Robot Adaptation for Effective Skill Transfer</title>
      <link>https://zamani.github.io/publication/icar2015/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/icar2015/</guid>
      <description>&lt;p&gt;













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/icar2015/featured2_hu84f6df181991a084387ec43fd16de8be_82756_fc5f5cd7a063e86319ad07d61ac347da.webp 400w,
               /publication/icar2015/featured2_hu84f6df181991a084387ec43fd16de8be_82756_e488f3cdab3523f4d43df09092770d5f.webp 760w,
               /publication/icar2015/featured2_hu84f6df181991a084387ec43fd16de8be_82756_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://zamani.github.io/publication/icar2015/featured2_hu84f6df181991a084387ec43fd16de8be_82756_fc5f5cd7a063e86319ad07d61ac347da.webp&#34;
               width=&#34;643&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/S3NW0hr72mU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting my work at BAILAR, ROMAN 2017</title>
      <link>https://zamani.github.io/talk/roman2017/</link>
      <pubDate>Sun, 27 Aug 2017 11:40:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/roman2017/</guid>
      <description>&lt;p&gt;I presented our work on &lt;a href=&#34;https://zamani.github.io/publication/roman2017/&#34;&gt;&lt;em&gt;Deep Reinforcement Learning using Symbolic Representation for Performing Spoken Language Instructions&lt;/em&gt;&lt;/a&gt; at 2nd Workshop on Behavior Adaptation, Interaction and Learning for Assistive Robotics (BAILAR). This workshop was part of the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seminar Talk</title>
      <link>https://zamani.github.io/talk/wtm_2017/</link>
      <pubDate>Sat, 17 Jun 2017 14:15:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/wtm_2017/</guid>
      <description>&lt;p&gt;Abstract: 
Spoken language is one of the most efficient ways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to successfully plan and execute the actions. We propose a system which can learn to recognize the user&amp;rsquo;s intention and map it to a goal for a reinforcement learning (RL) system. This system is then used to generate a sequence of actions toward this goal considering the state of the environment. Symbolic representations are used for both input and output of a Deep RL module. To show the effectiveness of our approach, the TellMeDave corpus is used to train the intention detection model and in a second step train the RL module towards the detected objective represented by a set of state predicates. We show that the system can successfully recognize instructions from this corpus and map them to the corresponding objective as well as train an RL system with symbolic input.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Seminar Talk</title>
      <link>https://zamani.github.io/talk/wtm_2016/</link>
      <pubDate>Tue, 31 May 2016 14:15:00 +0000</pubDate>
      <guid>https://zamani.github.io/talk/wtm_2016/</guid>
      <description>&lt;p&gt;Abstract: 
In the future, robots are expected to work as a companion with humans in various areas ranging from service robots to humanoid robots. Dynamic and unpredictable human/domestic environments force developers to improve safety for human-robot cooperation. One natural approach for humans is to warn about threats using natural spoken language. Then, robots should be able to modulate safer actions by a syntactic/semantic understanding of those warnings. In pour research deep neural networks will be used as the main learning approach of the natural language processing part. However, besides warning messages, other modalities seem necessary to gain a better understanding of threats such as prosody and vision. Generating safer actions depending on context can be performed by reinforcement learning or simply by choosing from an available action set. Moreover, possible tasks and scenarios as well as datasets and platforms will be discussed in this talk&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CONVERGENT Project</title>
      <link>https://zamani.github.io/project/convergent/</link>
      <pubDate>Sun, 23 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/project/convergent/</guid>
      <description>&lt;p&gt;I was part of the CONVERGENT project between 2012-2015. Here is The main page for the &lt;a href=&#34;https://robotics.ozyegin.edu.tr/convergent-human-learning-for-robot-skill-generation/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;CONVERGENT project&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/S3NW0hr72mU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/BtewbdaRQc0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://zamani.github.io/publication/example/__index/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/publication/example/__index/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://zamani.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://zamani.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
