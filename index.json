[{"authors":["zamani"],"categories":null,"content":"I am a Senior Machine Learning Applied Scientist at Hamburg Informatics Technology Center (HITeC) and a Research Associate at University of Hamburg. I am passionate to find scalable solutions based on deep learning for industrial problems. My research focus has been on deep (reinforcement) learning including explainability and its applications in robotics, dialogue management systems, and planning. I am also interested in AutoML as a path to reach a scalable artificial intelligence solution.\n","date":1579132800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1645488000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Senior Machine Learning Applied Scientist at Hamburg Informatics Technology Center (HITeC) and a Research Associate at University of Hamburg. I am passionate to find scalable solutions based on deep learning for industrial problems.","tags":null,"title":"Mohammad Ali Zamani","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://zamani.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"9c8bd45e052031f4455279f32cf66526","permalink":"https://zamani.github.io/project/videoscout/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/project/videoscout/","section":"project","summary":"VideoScout - Capturing a Full Football Game Using Video Footage.","tags":["Deep Learning"],"title":"VideoScout","type":"project"},{"authors":null,"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"49efda8aa2abc34ce5afd0bfd4fa0608","permalink":"https://zamani.github.io/project/verikas/","publishdate":"2022-02-22T00:00:00Z","relpermalink":"/project/verikas/","section":"project","summary":"VeriKAS – Verification of learning AI applications in the aviation sector.","tags":["Deep Learning"],"title":"VERIKAS","type":"project"},{"authors":[],"categories":null,"content":"In collaboration with ARIC, I had my second workshop on reinforcement learning. This workshop was followed by a hands-on session about RL.\n","date":1621944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621944000,"objectID":"356731f347cd9af42349a6d19c711187","permalink":"https://zamani.github.io/talk/rl_ws_2021/","publishdate":"2021-05-25T12:00:00Z","relpermalink":"/talk/rl_ws_2021/","section":"talk","summary":"In collaboration with ARIC, I had my second workshop on reinforcement learning. This workshop was followed by a hands-on session about RL.","tags":[],"title":"Workshop on Reinfocement Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613952000,"objectID":"750f1fa81222f6f26e47eec5f2b5bb6f","permalink":"https://zamani.github.io/project/impa/","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/project/impa/","section":"project","summary":"Intelligent Media Production Assistant.","tags":["Deep Learning"],"title":"IMPA","type":"project"},{"authors":[],"categories":null,"content":"I was invited as speaker to the Remote Brown Bag Sessions which are orginized by Artificial Intelligence Center Hamburg (ARIC). In this talk, I presented our work on Continuous Acoustic Emotion Classification which we could reduce latency and improve the accuracy for critical cases. You can find the list of talks orginized by ARIC here.\n","date":1597320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597320000,"objectID":"290bee1962aaac77bf077522524513ed","permalink":"https://zamani.github.io/talk/brown_bag_2020/","publishdate":"2020-08-13T12:00:00Z","relpermalink":"/talk/brown_bag_2020/","section":"talk","summary":"I was invited as speaker to the Remote Brown Bag Sessions which are orginized by Artificial Intelligence Center Hamburg (ARIC). In this talk, I presented our work on Continuous Acoustic Emotion Classification which we could reduce latency and improve the accuracy for critical cases.","tags":[],"title":"Continuous Acoustic Emotion Classification using Recurrent Neural Network","type":"talk"},{"authors":null,"categories":null,"content":"","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"60f4ddf5b937f975e143a03fdca1d73b","permalink":"https://zamani.github.io/project/unevis/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/project/unevis/","section":"project","summary":"UNEVIS - AI systems for marketing in the automotive industry .","tags":["Deep Learning"],"title":"UNEVIS - AI","type":"project"},{"authors":[],"categories":null,"content":"The (online) workshop is organized by Hamburger Informatik Technologie-Center e.V. (HITeC) and Artificial Intelligence Center Hamburg e.V. (ARIC) on 9th, 16th, and 23rd April 2020. The focus of the workshop was on optimizing hyperparameters of deep learning models with Bayesian optimization. In this workshop, participants learned how to write searchable deep learning models in Pytorch and searching for hyperparameters with one of the latest AutoML methods. Bayesian Optimization and HyperBand (BOHB) is introduced for hyperparameter optimization. On each day, there was a practical session for implementing the concepts on Google Colaboratory.\nThe content of the workshop was:\nDay 1 (09.04.2020) Introduction to Neural Network Basics of Neural Network Basics of Pytorch Multi-Layer Perceptron Hands-on Session Day 2 (16.04.2020) Convolutional Neural Network Questions from the previous session Basics of Convolutional Neural Network Convolutional Neural Network in Pytorch Hands-on Session Day 3 (23.04.2020) Bayesian Optimization Questions from the previous session Introduction of Bayesian Optimization Introduction of BOHB Framework Writing a Searchable Architecture Hands-on Session ","date":1587672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587672000,"objectID":"529c7519aa5c57a1508c1c70399826e2","permalink":"https://zamani.github.io/talk/aric2020/","publishdate":"2020-04-23T20:00:00Z","relpermalink":"/talk/aric2020/","section":"talk","summary":"The (online) workshop is organized by Hamburger Informatik Technologie-Center e.V. (HITeC) and Artificial Intelligence Center Hamburg e.V. (ARIC) on 9th, 16th, and 23rd April 2020. The focus of the workshop was on optimizing hyperparameters of deep learning models with Bayesian optimization.","tags":[],"title":"Workshop on Deep Neural Learning and Bayesian Optimization of Hyperparameters","type":"talk"},{"authors":[],"categories":null,"content":"My second internal workshop at HITeC was about deep learning with Pytorch and AutoML. I introduced the basics of deep learning and how to implement a classification problem in Pytorch. Then, I introduced one of the latest AutoML approach called BOHB for hyperparameter optimization. At the end of the workshop, we had hands-on session for hyperparameter search for a 2-D classification problem. The content of the workshop was:\nDeep Learning with Pytorch AutoML Hands-on Session ","date":1581071400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581071400,"objectID":"9f17315d1b4149ae61064f28cd6b4512","permalink":"https://zamani.github.io/talk/hitec_automl_2020/","publishdate":"2020-02-07T10:30:00Z","relpermalink":"/talk/hitec_automl_2020/","section":"talk","summary":"My second internal workshop at HITeC was about deep learning with Pytorch and AutoML. I introduced the basics of deep learning and how to implement a classification problem in Pytorch. Then, I introduced one of the latest AutoML approach called BOHB for hyperparameter optimization.","tags":[],"title":"Workshop on Deep Learning with Pytorch and AutoML","type":"talk"},{"authors":["Mohammad Ali Zamani"],"categories":["Codes"],"content":"Overview bike-sharing The goal is to predict the bike sharing count per hour. The dataset can be downloaded here\nI have uploaded the source code in this repo: https://github.com/zamani/bike_sharing_prediction\nSetup ⚠️update pip and install virtualenv if necessary (check here) (Optional) update the python pip\npython -m pip install --upgrade pip\rFirst clone the repo and navigate into the local repo:\ngit clone https://github.com/zamani/bike_sharing_prediction.git\rcd bike_sharing_prediction\rCreate a virtual environment in python:\npython -m venv bike_sharing_env\rThen, activate the environment on Windows:\n.\\bike_sharing_env\\Scripts\\activate\r(And on Linux/macOS)\nsource env/bin/activate\rFinally, install the requirements:\npip install -r requirements.txt\rBaselines The proposed RNN method is compared with some simple baseline methods:\nBaseline Method MAE STD Previous Hour Count 85.06 97.70 Previous Day, Same Hour 81.08 108.48 Keep Hourly Pace 89.21 125.17 Linear Regression, without regularization 142.21 115.59 The results can be shown by running the following command\npython baselines.py\rMethod In this repository, I have used RNN-GRU with different configurations. The RNN architecture is chosen for this problem because considering the sequence of consequence features and the previous output gives us additional information (comparing with only current input features). This leads to a more accurate prediction and can be modeled by the RNN. First, the dataset is divided into 70% training set, 10% validation set and 20% test set. Then, the bike count is normalized respect to the maximum bike count number in the training set.\nThe network is trained with the relative count respect to the previous day (same hour) or the previous hour. It means in this approach we have one of these two assumptions: either (1) we already know the count of the previous hour and we need to predict the next hour or (2) we know the previous day data (which is more realistic). The proposed architecture calculates the relative change and its value is converted into the absolute value for getting the absolute count number. Also, the relative change was trained by converting them into discrete bins and use the representative of bins as the relative increase/decrease. Only the number of bins should be defined. The bins width are automatically selected in a way that each bin almost has the same number of samples. On the other hands bins can also be defined manually such as [-1]+[-.75:0.05:0.45]+[0.5:0.25:2.5].\nHere are the results with Gated Recurrent Unit (GRU) on a fixed random seed:\nRNN-GRU input (output is Y(t)) command (python main.py \u0026lt;ARGUMENTS\u0026gt;) MAE STD X(t) -seqlen 1 82.42 132.94 [X(t-1), X(t)] -seqlen 2 75.53 140.15 [X(t-2), X(t-1), X(t)] -seqlen 3 92.69 162.90 [X(t), Y(t-1)] -seqlen 1 -prev_cnt hour 43.78 71.26 [[X(t-1), Y(t-2)], [X(t), Y(t-1)]] -seqlen 2 -prev_cnt hour 13.73 19.33 [X(t), Y(t-24)] -seqlen 1 -prev_cnt day -day_num 1 65.95 108.71 [X(t), Y(t-24), Y(t-48)] -seqlen 1 -prev_cnt day -day_num 2 33.15 33.15 [[X(t-1), Y(t-25)], [X(t), Y(t-24)]] -seqlen 2 -prev_cnt day -day_num 1 64.24 100.13 [[X(t-1), Y(t-25), Y(t-49)], [X(t), Y(t-24), Y(t-48)]] -seqlen 2 -prev_cnt day -day_num 2 18.98 34.17 The predicted value and actual value for a portion of test set is shown in the figure below (with this configuration [[X(t-1), Y(t-25), Y(t-49)], [X(t), Y(t-24), Y(t-48)]]).\nThere are other hyperparameters which can be explored such as learning rate, batch size, num of bins, hidden size, number of layers, dropout, and reduced features. Also, different values can be tested for seqlen, day_num.\n","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582329600,"objectID":"4df349f590accb70c2c3f2fca2f33e72","permalink":"https://zamani.github.io/post/bike_sharing_project/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/bike_sharing_project/","section":"post","summary":"The goal is to predict the bike sharing count per hour. The dataset can be found here (https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset.)","tags":["Codes"],"title":"Bike sharing prediction using deep learning","type":"post"},{"authors":["Mohammad Ali Zamani"],"categories":null,"content":"Word embedding In the sentiment analysis task, we can use either a trainable word embedding or a pretrained one. A pretrained word embedding such as Word2Vec or Glove has trained to capture semantic relationship between two words. It means if they are semantically close together, their word embedding vector is also similar (based on similarity measurement such as cosine similarity). So, this is expected to be helpful in a sentiment analysis task as we expect the similar words to have the same sentiment.\nOn the other hand, the trainable word embedding (or embedding layer) could be trained based on specific task e.g. sentiment analysis. Despite the pretrained word embedding, the trainable word embedding representation is randomly initialized at the begining of learning sentiment analysis. Then, eventually, the vectors are updated to organize the words sentimentally.\nVisualization The goal is to show that how a trainable word embedding is updated during the training process of sentiment analysis task. The embedding dimension is chosen as 2. Therefore, we can directly observe the changes in the word embedding without using any dimension reduction approach such t-SNE. The values of word embedding are averaged out to summarize every given sentence with a fixed 2 dimension.Then, it is connected to a 5 layer MLP (Fully connected). The number of layers are selected without any extensive hyper-parameter search.\nOnly few interesting words are selected to be observed during training. As can be seen, at the beginning (i.e. epoch 1), every word is randomly placed due to the random initialization. However, eventually, the positive words move to the one side of the graph and the negative words move to the other side. After, some epochs the selected words are linearly separable which makes the job for the deep learning part much easier. Although, it should be considered that the demo is only for a selected group of words.\n","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"9b6b9e2bf1204ad9daac270b98fe4462","permalink":"https://zamani.github.io/post/sentiment_analysis/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/post/sentiment_analysis/","section":"post","summary":"In this post, I want to visualize how does word embedding works in a sentiment analysis model. I have limited the word embedding dimension to 2 and visualize the position of word after each epoch.","tags":null,"title":"Word embeddings in Sentiment Analysis","type":"post"},{"authors":[],"categories":null,"content":"I had an internal workshop at HITeC on Introduction to Gaussian Process and Bayesian Optimization. The content of the workshop was:\nBasics from Statistics Gaussian Process Bayesian Optimization ","date":1578911400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578911400,"objectID":"1178f92b87ba083fb8c5a2b2ee7e3976","permalink":"https://zamani.github.io/talk/hitec_gpbo2020/","publishdate":"2020-01-13T10:30:00Z","relpermalink":"/talk/hitec_gpbo2020/","section":"talk","summary":"I had an internal workshop at HITeC on Introduction to Gaussian Process and Bayesian Optimization. The content of the workshop was:\nBasics from Statistics Gaussian Process Bayesian Optimization ","tags":[],"title":"Workshop on Gaussian Process and Bayesian Optimization","type":"talk"},{"authors":null,"categories":[],"content":"I was part of the SECURE project between 2016-2019. Here is The main page for the SECURE project\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"c1db8d14456f63296df246ada6182c9b","permalink":"https://zamani.github.io/project/secure/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/secure/","section":"project","summary":"Safety Enables Cooperation in Uncertain Robotic Environments","tags":["human-robot interaction","Deep Learning","Deep Reinforcement Learning","Robotics"],"title":"SECURE Project","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\rMath In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\rCustom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\rQuestions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://zamani.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Mohammad Ali Zamani","Sven Magg","Cornelius Weber","Di Fu","Stefan Wermter"],"categories":[],"content":"","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548633600,"objectID":"e2ba9c9c9899a01d2bde53d737f26dfb","permalink":"https://zamani.github.io/publication/paladyn/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/publication/paladyn/","section":"publication","summary":"Spoken language is one of the most efficient ways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to plan and execute actions successfully. We propose a system  that learns to recognise the user's intention and map it to a goal. A reinforcement learning (RL) system then generates a sequence of actions toward this goal considering the state of the environment. A novel contribution in this paper is the use of symbolic representations for both input and output of a neural Deep Q-network (DQN), which enables it to be used in a hybrid system. To show the effectiveness of our approach, the Tell-Me-Dave corpus is used to train an intention detection model and in a second step an RL agent generates the sequences of actions towards the detected objective, represented by a set of state predicates. We show that the system can successfully recognise command sequences from this corpus as well as train the deep-RL network with symbolic input. We further show that the performance can be significantly increased by exploiting the symbolic representation to generate intermediate rewards.","tags":["Reinforcement Learning","Spoken Language Instruction"],"title":"Deep reinforcement learning using compositional representations for performing instructions","type":"publication"},{"authors":["Egor Lakomkin","Mohammad Ali Zamani","Cornelius Weber","Sven Magg","Stefan Wermter"],"categories":[],"content":" ","date":1548374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548374400,"objectID":"1dd6da9780f03195ab9de46102b917b9","permalink":"https://zamani.github.io/publication/icra2019/","publishdate":"2019-01-25T00:00:00Z","relpermalink":"/publication/icra2019/","section":"publication","summary":"Previous work on emotion recognition demonstrated a synergistic effect of combining several modalities such as auditory, visual, and transcribed text, to estimate the affective state of a speaker. Among these, the linguistic modality is crucial for the evaluation of an expressed emotion. However, manually transcribed spoken text cannot be given as input to a system practically. We argue that using ground truth transcriptions during training and evaluation phases leads to a significant discrepancy in performance compared to real-world conditions, as the spoken text has to be recognized on the fly and can contain speech recognition mistakes. In this paper, we propose a method of integrating an automatic speech recognition (ASR) output with a character-level recurrent neural network for sentiment recognition. In addition, we conduct several experiments investigating sentiment recognition in human-robot interaction in a noise realistic scenario which is challenging for the ASR systems. We quantify the improvement compared to using only the acoustic modality in sentiment recognition. We demonstrate the effectiveness of this approach on the Multimodal Corpus of Sentiment Intensity (MOSI) by achieving 73,6% accuracy in a binary sentiment classification task, exceeding previously reported results that use only acoustic input. In addition, we set a new state-of-the-art performance on the MOSI dataset (80.4% accuracy, 2% absolute improvement).","tags":["Sentiment Recognition","Human-Robot Interaction","Emotion Recognition"],"title":"Incorporating End-to-End Speech Recognition Models for Sentiment Analysis","type":"publication"},{"authors":null,"categories":[],"content":"We demonstrated our project \u0026#34; robot that learns how to perform language instructions\u0026#34; in ‘Science is Wonder-ful!’, on September 26-27 2018. Our project shows the ability of robots to understand spoken instructions which was working a noisy condition.You can see more details here.\nI explained our project to Dimitrios Papadimoulis, Vice President of the European Parliament. (Image credit: European Comission - MSCA)\nYou can find the video here: ","date":1543968305,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968305,"objectID":"cdd4005d1eb241c36b8e03d92aecab42","permalink":"https://zamani.github.io/outreach/msca/","publishdate":"2018-12-05T01:05:05+01:00","relpermalink":"/outreach/msca/","section":"outreach","summary":"We demonstrated our project \" robot that learns how to perform language instructions\" in ‘Science is Wonder-ful!’, on September 26-27 2018. Our project shows the ability of robots to understand spoken instructions which was working a noisy condition.","tags":["Manipulation","ROS","Arm robot"],"title":"1) European Researchers' Night:","type":"outreach"},{"authors":[],"categories":null,"content":"I jointly presented our paper On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks during the interactive section. Also, on the last day of the conference, I attended the Machine Learning in Robot Motion Planning workshop to present our work on Neural End-to-End Learning of Reach for Grasp Ability with a 6-DoF Robot Arm.\n","date":1538757000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538757000,"objectID":"1c0014241a78917551b180419097fb42","permalink":"https://zamani.github.io/talk/iros2018/","publishdate":"2018-10-05T16:30:00Z","relpermalink":"/talk/iros2018/","section":"talk","summary":"I jointly presented our paper On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks during the interactive section. Also, on the last day of the conference, I attended the Machine Learning in Robot Motion Planning workshop to present our work on Neural End-to-End Learning of Reach for Grasp Ability with a 6-DoF Robot Arm.","tags":[],"title":"2 poster presentations at IROS 2018","type":"talk"},{"authors":["Hadi Beik-Mohammadi","Matthias Kerzel","Michael Goerner","Mohammad Ali Zamani","Manfred Eppe","Stefan Wermter"],"categories":[],"content":" ","date":1538697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538697600,"objectID":"a54de7971bc2c26d9c33ccc5f800a4e8","permalink":"https://zamani.github.io/publication/iros2018_ws/","publishdate":"2018-10-05T00:00:00Z","relpermalink":"/publication/iros2018_ws/","section":"publication","summary":"We present a neural end-to-end learning approach for a reach-for-grasp task on an industrial UR5 arm. Our approach combines the generation of suitable training samples by classical inverse kinematics (IK) solvers in a simulation environment in conjunction with real images taken from the grasping setup. Samples are generated in a safe and reliable way independent of real robotic hardware. The neural architecture is based on a pre-trained VGG16 network and trained on our collected images as input and motor joint values as output. The approach is evaluated by testing the performance on two test sets of different complexity levels. Based on our results, we outline challenges and solutions when combining classical and neural visuomotor approaches.","tags":["Reinforcement Learning","Robot Manipulation"],"title":"Neural End-to-End Learning of Reach for Grasp Ability with a 6-DoF Robot Arm","type":"publication"},{"authors":["Egor Lakomkin","Mohammad Ali Zamani","Cornelius Weber","Sven Magg","Stefan Wermter"],"categories":[],"content":"","date":1538611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538611200,"objectID":"b4d60e8b6fe785b4ccfe8ff800d6e8c0","permalink":"https://zamani.github.io/publication/iros2018/","publishdate":"2018-10-04T00:00:00Z","relpermalink":"/publication/iros2018/","section":"publication","summary":"Speech emotion recognition (SER) is an important aspect of effective human-robot collaboration and received a lot of attention from the research community. For example, many neural network-based architectures were proposed recently and pushed the performance to a new level. However, the applicability of such neural SER models trained only on in-domain data to noisy conditions is currently under-researched. In this work, we evaluate the robustness of state-of-the-art neural acoustic emotion recognition models in human-robot interaction scenarios. We hypothesize that a robot's ego noise, room conditions, and various acoustic events that can occur in a home environment can significantly affect the performance of a model. We conduct several experiments on the iCub robot platform and propose several novel ways to reduce the gap between the model's performance during training and testing in real-world conditions. Furthermore, we observe large improvements in the model performance on the robot and demonstrate the necessity of introducing several data augmentation techniques like overlaying background noise and loudness variations to improve the robustness of the neural approaches.","tags":["Human-Robot Interaction","Emotion Recognition"],"title":"On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks","type":"publication"},{"authors":["Mohammad Ali Zamani","Sven Magg","Cornelius Weber","Stefan Wermter"],"categories":[],"content":"","date":1538179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538179200,"objectID":"8895e77b79026122628e6abfbcf3b0a9","permalink":"https://zamani.github.io/publication/ssr2018/","publishdate":"2018-09-29T00:00:00Z","relpermalink":"/publication/ssr2018/","section":"publication","summary":"Spoken language can be an efficient and intuitive way to warn robots about threats. Guidance and warnings from a human can be used to inform and modulate a robot’s actions. An open research question is how the instructions and warnings can be integrated in the planning of the robot to improve safety. Our goal is to address this problem by defining a Deep Reinforcement Learning (DRL) agent to determine the intention of a given spoken instruction, especially in a domestic task, and generate a high-level sequence of actions to fulfill the given instruction. The DRL agent will combine vision and language to create a multi-modal state representation of the environment. We will also focus on how warnings can be used to shape the DRL’s reward, concentrating on the recognition the emotional state of the human in an interaction with the robot. Finally, we will use language instructions to determine safe operational space for the robot.","tags":["Human-Robot Interaction","Reinforcement Learning","Natural Language Processing"],"title":"Language-modulated Actions using Deep Reinforcement Learning for Safer Human-Robot Interaction","type":"publication"},{"authors":[],"categories":null,"content":"We organized a PhD students conference focused safety and interaction quality. I also had a chance to present my progress in this conference.\n","date":1538144100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538144100,"objectID":"811a90daaba4d2b0b036a54edf3b010c","permalink":"https://zamani.github.io/talk/ssr2018/","publishdate":"2018-09-28T14:15:00Z","relpermalink":"/talk/ssr2018/","section":"talk","summary":"We organized a PhD students conference focused safety and interaction quality. I also had a chance to present my progress in this conference.","tags":[],"title":"Presenting my research progress","type":"talk"},{"authors":["Mohammad Ali Zamani","Hadi Beik-Mohammadi","Matthias Kerzel","Sven Magg","Stefan Wermter"],"categories":[],"content":" ","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"8e4f817514fdcbc3c1064e1c00576945","permalink":"https://zamani.github.io/publication/icra2018_workmate_ws/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/publication/icra2018_workmate_ws/","section":"publication","summary":"Programming robots for a safe interaction with humans is extremely complex especially in collaborative tasks. One reason is the unpredictable behaviour of humans that may have an intention which is not clear to the robot. We present a novel architecture for a safe human-robot collaboration scenario in a shared tabletop workspace based on intuitive multimodal language and gesture instructions and behaviour recognition. In our example scenario, a human and a robot arm collaboratively have to assemble a Tangram puzzle. The configuration space of the robot is constrained by a combination of learned behaviour patterns of the user by tracking its arm and direct audio-visual instructions regarding the sharing of the workspace. This ensures a safe and non-obstructive collaboration behavior of the robot which can constantly be updated during task execution. In this paper, we present initial results with a focus on instruction understanding.","tags":["Reinforcement Learning","Robot Manipulation","demos"],"title":"Learning Spatial Representation for Safe Human-Robot Collaboration in Joint Manual Tasks","type":"publication"},{"authors":[],"categories":null,"content":" I was interviewed by the MCAA IRRADIUM magazine about the challenges, development, and uptake of . The online version of the interview can be found here:\nEuropean approach to Artificial Intelligence: Challenges, developments and uptake\nThe PDF version of the magazine can be downloaded here (page 16-17)\n","date":1527001200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527001200,"objectID":"120025baf67193f95dc234dc88bf5262","permalink":"https://zamani.github.io/outreach/irradium/","publishdate":"2018-05-22T15:00:00Z","relpermalink":"/outreach/irradium/","section":"outreach","summary":"I was interviewed by the MCAA IRRADIUM magazine about the challenges, development, and uptake of . The online version of the interview can be found here:\nEuropean approach to Artificial Intelligence: Challenges, developments and uptake","tags":[],"title":"2) Interview with the MCAA IRRADIUM magazine","type":"outreach"},{"authors":[],"categories":null,"content":"I presented 3 posters at ICRA main conference, Workmate workshop and PhD forum.\nThe first presentation was “Emorl: Continuous acoustic emotion classification using deep reinforcement learning” at ICRA 2018.\nI also presented “Learning spatial representation for safe human-robot collaboration in joint manual tasks” at the Workmate workshop at ICRA 2018.\nAnd finally, I presented my work on “Language-modulated Actions for Safer Human-Robot Interaction using Deep Reinforcement Learning” as a poster in the ICRA 2018 Ph.D. forum.\n","date":1527001200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527001200,"objectID":"d16f7a5d46b8c91418bb519cea7012a4","permalink":"https://zamani.github.io/talk/icra2018/","publishdate":"2018-05-22T15:00:00Z","relpermalink":"/talk/icra2018/","section":"talk","summary":"I presented 3 posters at ICRA main conference, Workmate workshop and PhD forum.\nThe first presentation was “Emorl: Continuous acoustic emotion classification using deep reinforcement learning” at ICRA 2018.\nI also presented “Learning spatial representation for safe human-robot collaboration in joint manual tasks” at the Workmate workshop at ICRA 2018.","tags":[],"title":"3 poster presentation at ICRA 2018","type":"talk"},{"authors":["Egor Lakomkin*","Mohammad Ali Zamani*","Cornelius Weber","Sven Magg","Stefan Wermter"],"categories":[],"content":" ","date":1526947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526947200,"objectID":"573548db9217a0b670736dd3a4aad30f","permalink":"https://zamani.github.io/publication/emorl/","publishdate":"2018-05-22T00:00:00Z","relpermalink":"/publication/emorl/","section":"publication","summary":"Acoustically expressed emotions can make communication with a robot more efficient. Detecting emotions like anger could provide a clue for the robot indicating unsafe/undesired situations. Recently, several deep neural network-based models have been proposed which establish new state-of-the-art results in affective state evaluation. These models typically start processing at the end of each utterance, which not only requires a mechanism to detect the end of an utterance but also makes it difficult to use them in a real-time communication scenario, e.g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.","tags":["Reinforcement Learning","Human-Robot Interaction","Emotion Recognition","demos"],"title":"EmoRL: Real-time Acoustic Emotion Classification using Deep Reinforcement Learning","type":"publication"},{"authors":["Mohammad Ali Zamani","Sven Magg","Cornelius Weber","Stefan Wermter"],"categories":[],"content":" ","date":1526774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526774400,"objectID":"7b26ea427859aada6c7fdd7256add2e2","permalink":"https://zamani.github.io/publication/icra2018_phd_forum/","publishdate":"2018-05-20T00:00:00Z","relpermalink":"/publication/icra2018_phd_forum/","section":"publication","summary":"Programming robots for a safe interaction with humans is extremely complex especially in collaborative tasks. One reason is the unpredictable behaviour of humans that may have an intention which is not clear to the robot. We present a novel architecture for a safe human-robot collaboration scenario in a shared tabletop workspace based on intuitive multimodal language and gesture instructions and behaviour recognition. In our example scenario, a human and a robot arm collaboratively have to assemble a Tangram puzzle. The configuration space of the robot is constrained by a combination of learned behaviour patterns of the user by tracking its arm and direct audio-visual instructions regarding the sharing of the workspace. This ensures a safe and non-obstructive collaboration behavior of the robot which can constantly be updated during task execution. In this paper, we present initial results with a focus on instruction understanding.","tags":["Reinforcement Learning","Robot Manipulation"],"title":"Language-modulated Safer Actions using Deep Reinforcement Learning","type":"publication"},{"authors":["Hadi Beik-Mohammadi","Matthias Kerzel","Mohammad Ali Zamani","Stefan Wermter"],"categories":null,"content":" ","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"6ab4d5de82065ced92faabc51cd99c22","permalink":"https://zamani.github.io/publication/ijcnn2018/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/publication/ijcnn2018/","section":"publication","summary":"Robotic motor policies can, in theory, be learned via deep continuous reinforcement learning. In practice, however, collecting the enormous amount of required training samples in realistic time, surpasses the possibilities of many robotic platforms. To address this problem, we propose a novel method for accelerating the learning process by task simplification inspired by the Goldilocks effect known from developmental psychology. We present results on a reachfor-grasp task that is learned with the Deep Deterministic Policy Gradients (DDPG) algorithm. Task simplification is realized by initially training the system with “larger-thanlife” training objects that adapt their reachability dynamically during training. We achieve a significant acceleration compared to the unaltered training setup. We describe modifications to the DDPG algorithm with regard to the replay buffer to prevent artifacts during the learning process from the simplified learning instances while maintaining the speed of learning. With this result, we contribute towards the realistic application of deep reinforcement learning on robotic platforms.","tags":["Reinforcement Learning"],"title":"Accelerating Deep Continuous Reinforcement Learning through Task Simplification","type":"publication"},{"authors":null,"categories":[],"content":"Click here for more photos\nSpoken language is potentially the most intuitive way to communicate with robot. In this project, we assembled an arm robot to demonstrate to public, especially younger kids, how they can instruct robot to perform actions. This project includes automatic speech recognition (ASR), text to speech, spoken language understanding, vision, and motion planning.\nTo work with our robot, a user can wake up the robot by its name “Jarvis”. Then, the robot confirms that is ready to receive a new command by saying “Yes!”. After hearing the confirmation, the user give a command such as “Pick cube number 5 and put it near cube numebr 1”. Although, we had our own domain-specific ASR model, due to operating condition which was in an extremely noisy condition we used the Google ASR. We ask the Google ASR the top most probable hypotheses. These hypotheses are used to identify the intended cubes as well as a target position for the picked cube. If there was no target position is commanded then robot performed a handover when users says “Release!”.\nUsing a top camera robot locates the cubes using the April tags on top each cubes and using moveIt! package it plans and performs the trajectory to reach to the cube. Jarvis can grasp the cube using a electomagnet which is installed at the end-effector of the robot. There is also a metal piece is attached to the top of the cubes.\n","date":1516665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516665600,"objectID":"1e18c5959bdcfcbd3d4a7e732a68564f","permalink":"https://zamani.github.io/project/instructing_arm_robot/","publishdate":"2018-01-23T00:00:00Z","relpermalink":"/project/instructing_arm_robot/","section":"project","summary":"Presented at European Researchers' Night in the Parlamentarium, Brussels, Belgium.","tags":["human-robot interaction","Robotics","Deep Learning"],"title":"Robot that performs language instructions","type":"project"},{"authors":[],"categories":null,"content":"In this presentation, I talked about AI and public concerns. The purpose of the talk was to give some intuition for a public audience that what is the research focus and methodological tools in AI.\n","date":1513359000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513359000,"objectID":"2b79c46c928762d27b5e872c48bfd868","permalink":"https://zamani.github.io/talk/ham2018/","publishdate":"2017-12-15T17:30:00Z","relpermalink":"/talk/ham2018/","section":"talk","summary":"In this presentation, I talked about AI and public concerns. The purpose of the talk was to give some intuition for a public audience that what is the research focus and methodological tools in AI.","tags":[],"title":"Should We Be Afraid of Artificial Intelligence?","type":"talk"},{"authors":["Mohammad Ali Zamani","Sven Magg","Cornelius Weber","Stefan Wermter"],"categories":[],"content":" ","date":1506556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506556800,"objectID":"ee8725db619578e8bfbdb05d49461cd6","permalink":"https://zamani.github.io/publication/roman2017/","publishdate":"2017-09-28T00:00:00Z","relpermalink":"/publication/roman2017/","section":"publication","summary":"Spoken language is one of the most efficient ways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to plan and execute the actions successfully. We propose a system which can learn to recognise the user’s intention and map it to a goal for a reinforcement learning (RL) system. This system is then used to generate a sequence of actions toward this goal considering the state of the environment. The novelty is the use of symbolic representations for both input and output of a neural Deep Q-network which enables it to be used in a hybrid system. To show the effectiveness of our approach, the Tell Me Dave corpus is used to train the intention detection model and in a second step to train the RL module towards the detected objective, represented by a set of state predicates. We show that the system can successfully recognise command sequences from this corpus as well as train the deep-RL network with symbolic input. We further show that the performance can be significantly increased by exploiting the symbolic representation to generate intermediate rewards.","tags":["Reinforcement Learning","Spoken Language Instruction"],"title":"Deep Reinforcement Learning using Symbolic Representation for Performing Spoken Language Instructions","type":"publication"},{"authors":["Mohammad Ali Zamani","Erhan Oztop"],"categories":[],"content":" ","date":1506556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506556800,"objectID":"eb0e96b039e68fbf45362b1a27de20f6","permalink":"https://zamani.github.io/publication/icar2015/","publishdate":"2017-09-28T00:00:00Z","relpermalink":"/publication/icar2015/","section":"publication","summary":"In this paper, we propose and implement a human-in-the loop robot skill synthesis framework that involves simultaneous adaptation of the human and the robot. In this framework, the human demonstrator learns to control the robot in real-time to make it perform a given task. At the same time, the robot learns from the human guided control creating a non-trivial coupled dynamical system. The research question we address is how this system can be tuned to facilitate faster skill transfer or improve the performance level of the transferred skill. In the current paper we report our initial work for the latter. At the beginning of the skill transfer session, the human demonstrator controls the robot exclusively as in teleoperation. As the task performance improves the robot takes increasingly more share in control, eventually reaching full autonomy. The proposed framework is implemented and shown to work on a physical cart-pole setup. To assess whether simultaneous learning has advantage over the standard sequential learning (where the robot learns from the human observation but does not interfere with the control) experiments with two groups of subjects were performed. The results indicate that the final autonomous controller obtained via simultaneous learning has a higher performance measured as the average deviation from the upright posture of the pole.","tags":["Human-robot Interaction","Human-in-the-loop","Imitation Learning","Learning by Demonstration","demos"],"title":"Simultaneous Human-Robot Adaptation for Effective Skill Transfer","type":"publication"},{"authors":[],"categories":null,"content":"I presented our work on Deep Reinforcement Learning using Symbolic Representation for Performing Spoken Language Instructions at 2nd Workshop on Behavior Adaptation, Interaction and Learning for Assistive Robotics (BAILAR). This workshop was part of the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN).\n","date":1503834000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503834000,"objectID":"a4e3fc359e4e7b824b87f131c7019eb1","permalink":"https://zamani.github.io/talk/roman2017/","publishdate":"2017-08-27T11:40:00Z","relpermalink":"/talk/roman2017/","section":"talk","summary":"I presented our work on Deep Reinforcement Learning using Symbolic Representation for Performing Spoken Language Instructions at 2nd Workshop on Behavior Adaptation, Interaction and Learning for Assistive Robotics (BAILAR). This workshop was part of the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN).","tags":[],"title":"Presenting my work at BAILAR, ROMAN 2017","type":"talk"},{"authors":[],"categories":null,"content":"Abstract: Spoken language is one of the most efficient ways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to successfully plan and execute the actions. We propose a system which can learn to recognize the user’s intention and map it to a goal for a reinforcement learning (RL) system. This system is then used to generate a sequence of actions toward this goal considering the state of the environment. Symbolic representations are used for both input and output of a Deep RL module. To show the effectiveness of our approach, the TellMeDave corpus is used to train the intention detection model and in a second step train the RL module towards the detected objective represented by a set of state predicates. We show that the system can successfully recognize instructions from this corpus and map them to the corresponding objective as well as train an RL system with symbolic input.\n","date":1497708900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497708900,"objectID":"50920c4f815d83fa6b93a4731cd76b45","permalink":"https://zamani.github.io/talk/wtm_2017/","publishdate":"2017-06-17T14:15:00Z","relpermalink":"/talk/wtm_2017/","section":"talk","summary":"Abstract: Spoken language is one of the most efficient ways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to successfully plan and execute the actions.","tags":[],"title":"Seminar Talk","type":"talk"},{"authors":[],"categories":null,"content":"Abstract: In the future, robots are expected to work as a companion with humans in various areas ranging from service robots to humanoid robots. Dynamic and unpredictable human/domestic environments force developers to improve safety for human-robot cooperation. One natural approach for humans is to warn about threats using natural spoken language. Then, robots should be able to modulate safer actions by a syntactic/semantic understanding of those warnings. In pour research deep neural networks will be used as the main learning approach of the natural language processing part. However, besides warning messages, other modalities seem necessary to gain a better understanding of threats such as prosody and vision. Generating safer actions depending on context can be performed by reinforcement learning or simply by choosing from an available action set. Moreover, possible tasks and scenarios as well as datasets and platforms will be discussed in this talk\n","date":1464704100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464704100,"objectID":"00c164b24c305542b26238dad55f14a6","permalink":"https://zamani.github.io/talk/wtm_2016/","publishdate":"2016-05-31T14:15:00Z","relpermalink":"/talk/wtm_2016/","section":"talk","summary":"Abstract: In the future, robots are expected to work as a companion with humans in various areas ranging from service robots to humanoid robots. Dynamic and unpredictable human/domestic environments force developers to improve safety for human-robot cooperation.","tags":[],"title":"Seminar Talk","type":"talk"},{"authors":null,"categories":[],"content":"I was part of the CONVERGENT project between 2012-2015. Here is The main page for the CONVERGENT project\n","date":1440288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440288000,"objectID":"c1e3b464affc5d19b5a026de5ec888cd","permalink":"https://zamani.github.io/project/convergent/","publishdate":"2015-08-23T00:00:00Z","relpermalink":"/project/convergent/","section":"project","summary":"Human Learning for Robot Skill GenerationT","tags":["human-robot interaction","Robotics"],"title":"CONVERGENT Project","type":"project"},{"authors":["Mohammad Ali Zamani"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"f53b6a5013dcfac4578af27a1869c8a9","permalink":"https://zamani.github.io/publication/example/__index/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/__index/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://zamani.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]